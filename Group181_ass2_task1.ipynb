{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCbAmQ47iqK4"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# FIT5196 Task 1 in Assessment 2\n",
    "#### Student Name: Deshui Yu      Liangjing Yang\n",
    "#### Student ID: 34253599      34060871\n",
    "\n",
    "Date: 28/09/2024\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjBFqYK4iqK5"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>    \n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Patent Files](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Defining Regular Expressions](#Reg_Exp) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Reading Files](#Read) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Whatever else](#latin) <br>\n",
    "[5. Writing to CSV/JSON File](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Verification - using the sample files](#test_xml) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcbqK3KliqK6"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEFdSCIUiqK6"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project involves cleansing and analyzing a retail transactional dataset from DigiCO, an online electronics store in Melbourne. The task is to detect and fix errors, impute missing values, and remove outliers using exploratory data analysis (EDA). Cleaned data will be saved in the required output files, and the process will be documented in the final report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:\n",
    "\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** to manage and analyze data.\n",
    "* **datetime** to handle dates and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import ast\n",
    "from collections import Counter\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 3.  Examining Raw Data <a class=\"anchor\" name=\"examine\"></a>\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到这三个文件都包含以下数据列：order_id、customer_id、date、nearest_warehouse、shopping_cart、order_price、delivery_charges、customer_lat、customer_long、coupon_discount、order_total、season、is_expedited_delivery、distance_to_nearest_warehouse、latest_customer_review 和 is_happy_customer。其中，coupon_discount、delivery_charges、shopping_cart 中的商品数量、order_id、customer_id 和 latest_customer_review 是没有错误的数据。在 Group181_missing_data.csv 文件中，is_happy_customer 列的数据缺失，其数据类型为 float64；在 Group181_dirty_data.csv 中存在错误数据；而在 Group181_outlier_data.csv 文件中则有异常数据。\n",
    "通过数据逻辑我们可以发现，date和season数据是有关系的，customer_lat and customer_long 和 distance_to_nearest_warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_file_path = 'Group181_dirty_data.csv'\n",
    "dirty_data = pd.read_csv(dirty_file_path)\n",
    "dirty_data['error'] = 0\n",
    "# dirty_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 16 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   order_id                       500 non-null    object \n",
      " 1   customer_id                    500 non-null    object \n",
      " 2   date                           500 non-null    object \n",
      " 3   nearest_warehouse              445 non-null    object \n",
      " 4   shopping_cart                  500 non-null    object \n",
      " 5   order_price                    485 non-null    float64\n",
      " 6   delivery_charges               460 non-null    float64\n",
      " 7   customer_lat                   500 non-null    float64\n",
      " 8   customer_long                  500 non-null    float64\n",
      " 9   coupon_discount                500 non-null    int64  \n",
      " 10  order_total                    485 non-null    float64\n",
      " 11  season                         500 non-null    object \n",
      " 12  is_expedited_delivery          500 non-null    bool   \n",
      " 13  distance_to_nearest_warehouse  469 non-null    float64\n",
      " 14  latest_customer_review         500 non-null    object \n",
      " 15  is_happy_customer              460 non-null    float64\n",
      "dtypes: bool(1), float64(7), int64(1), object(7)\n",
      "memory usage: 59.2+ KB\n"
     ]
    }
   ],
   "source": [
    "missing_file_path = 'Group181_missing_data.csv'\n",
    "missing_data = pd.read_csv(missing_file_path)\n",
    "missing_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_file_path = 'Group181_outlier_data.csv'\n",
    "outlier_data = pd.read_csv(outlier_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 4.  Detect and fix errors in dirty_data <a class=\"anchor\" name=\"load\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.1. Fix the date <a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through examining the current data, the issue was identified as the date values, which were supposed to be in the YYYY-MM-DD format, mistakenly being formatted as YYYY-DD-MM and DD-MM-YYYY. Additionally, based on logical reasoning, the date and season are highly correlated, so after fixing the date data, the season data should also be corrected accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and mark invalid dates\n",
    "# reference from chatGPT\n",
    "# Temporarily convert the 'date' column to datetime format, marking invalid dates as NaT (Not a Time)\n",
    "temp_dates = pd.to_datetime(dirty_data['date'], errors='coerce')\n",
    "# Find the invalid date\n",
    "invalid_dates_temp = dirty_data[temp_dates.isna()]\n",
    "# print(invalid_dates_temp[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([33, 43, 69, 110, 172, 240, 246, 285, 291, 371, 443, 466, 467], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# 将 date 列转换为 datetime，错误值会变成 NaT\n",
    "temp_dates = pd.to_datetime(dirty_data['date'], errors='coerce')\n",
    "\n",
    "# 将 NaT 行标记为 1，表示有错误\n",
    "dirty_data.loc[temp_dates.isna(), 'error'] = 1\n",
    "\n",
    "# 获取 dirty_data 中 error 列标记为 1 的行号\n",
    "error_indices = dirty_data.index[dirty_data['error'] == 1]\n",
    "\n",
    "# 打印出这些行号\n",
    "print(error_indices)\n",
    "\n",
    "# 准备修复日期并标记修复\n",
    "fixed_dates = []\n",
    "for idx, date_str in enumerate(dirty_data['date'].astype(str)):\n",
    "    parts = date_str.split('-')\n",
    "    # Check if format is DD-MM-YYYY and fix to YYYY-MM-DD\n",
    "    if len(parts) == 3 and int(parts[0]) <= 12 and int(parts[1]) <= 31:\n",
    "        fixed_dates.append(f'{parts[2]}-{parts[1]}-{parts[0]}')  # DD-MM-YYYY -> YYYY-MM-DD\n",
    "        dirty_data.loc[idx, 'error'] = 2  # 只标记当前行\n",
    "    # Check if format is YYYY-DD-MM and fix to YYYY-MM-DD\n",
    "    elif len(parts) == 3 and int(parts[1]) > 12 and int(parts[2]) <= 12:\n",
    "        fixed_dates.append(f'{parts[0]}-{parts[2]}-{parts[1]}')  # YYYY-DD-MM -> YYYY-MM-DD\n",
    "        dirty_data.loc[idx, 'error'] = 2  # 只标记当前行\n",
    "    # Keep original format for other cases\n",
    "    else:\n",
    "        fixed_dates.append(date_str)\n",
    "\n",
    "# 将修复后的日期转换回 datetime 格式，并替换原始的 'date' 列\n",
    "dirty_data['date'] = pd.to_datetime(fixed_dates, errors='coerce')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.2. Fix the season <a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the date and season data are logically related, once the date data has been corrected, the season data should also be updated accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([  8,  24,  57,  60,  66,  95, 100, 115, 116, 132, 138, 147, 195,\n",
      "            199, 211, 212, 213, 222, 245, 250, 255, 267, 323, 446, 462, 468,\n",
      "            496],\n",
      "           dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "dirty_data['date'] = pd.to_datetime(dirty_data['date'], errors='coerce')\n",
    "\n",
    "# 定义获取季节的函数\n",
    "def get_correct_season(month):\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Summer'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Autumn'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Winter'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Spring'\n",
    "\n",
    "# 第一步：标记错误的 season 数据\n",
    "\n",
    "# 提取没有标记 error 的数据行\n",
    "row_season_data = dirty_data[dirty_data['error'] == 0].copy()\n",
    "\n",
    "# 提取月份\n",
    "row_season_data['month'] = row_season_data['date'].dt.month\n",
    "\n",
    "# 应用获取正确季节的函数\n",
    "row_season_data['correct_season'] = row_season_data['month'].apply(get_correct_season)\n",
    "\n",
    "# 检查 season 列是否正确\n",
    "row_season_data['season_is_wrong'] = row_season_data['season'] != row_season_data['correct_season']\n",
    "\n",
    "# 获取 season 错误的行的索引\n",
    "wrong_season_indices = row_season_data.index[row_season_data['season_is_wrong']]\n",
    "\n",
    "# 将对应的行在 dirty_data 中标记为 error\n",
    "dirty_data.loc[wrong_season_indices, 'error'] = 1\n",
    "\n",
    "# 删除临时列，并忽略不存在的列错误\n",
    "dirty_data.drop(columns=['month', 'correct_season', 'season_is_wrong'], inplace=True, errors='ignore')\n",
    "\n",
    "\n",
    "# 获取 dirty_data 中 error 列标记为 1 的行号\n",
    "error_indices = dirty_data.index[dirty_data['error'] == 1]\n",
    "\n",
    "# 打印出这些行号\n",
    "print(error_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  season\n",
      "0   2019-01-23  Summer\n",
      "1   2019-11-07  Spring\n",
      "2   2019-01-14  Summer\n",
      "3   2019-10-31  Spring\n",
      "4   2019-04-02  Autumn\n",
      "..         ...     ...\n",
      "495 2019-11-03  Spring\n",
      "496 2019-02-07  Autumn\n",
      "497 2019-05-29  Autumn\n",
      "498 2019-05-03  Autumn\n",
      "499 2019-09-06  Spring\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "fixed_seasons = []\n",
    "\n",
    "# 遍历每一行，修正季节数据\n",
    "for idx, date in dirty_data.iterrows():\n",
    "    month = date['date'].month  # 提取月份\n",
    "    # 如果 error 列为 1，则修正季节\n",
    "    if dirty_data.loc[idx, 'error'] == 1:\n",
    "        if month in [9, 10, 11]:\n",
    "            fixed_seasons.append('Spring')\n",
    "        elif month in [12, 1, 2]:\n",
    "            fixed_seasons.append('Summer')\n",
    "        elif month in [3, 4, 5]:\n",
    "            fixed_seasons.append('Autumn')\n",
    "        elif month in [6, 7, 8]:\n",
    "            fixed_seasons.append('Winter')\n",
    "        # 将错误标记修正为 2\n",
    "        dirty_data.loc[idx, 'error'] = 2\n",
    "    else:\n",
    "        # 如果 error 不为 1，则保留原来的 season 值\n",
    "        fixed_seasons.append(dirty_data.loc[idx, 'season'])\n",
    "\n",
    "# 将修复后的 season 列替换掉原始的 season 列\n",
    "dirty_data['season'] = fixed_seasons\n",
    "\n",
    "# 打印修复后的 date 和 season 列\n",
    "print(dirty_data[['date', 'season']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.3. Fix the customer_lat, customer_long<a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([ 48,  68,  77,  87,  93, 136, 137, 140, 187, 196, 198, 216, 220,\n",
      "            230, 242, 297, 299, 361, 366, 393, 408, 427, 438, 452, 464, 469,\n",
      "            495],\n",
      "           dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "for idx, row in dirty_data.iterrows():\n",
    "    lat_issue = row['customer_lat'] > 0  # 纬度应为负值\n",
    "    long_issue = row['customer_long'] < 0  # 经度应为正值\n",
    "\n",
    "    # 如果存在纬度问题或经度问题，则将 error 列标记为 1\n",
    "    if lat_issue or long_issue:\n",
    "        dirty_data.loc[idx, 'error'] = 1\n",
    "        \n",
    "# 获取 dirty_data 中 error 列标记为 1 的行号\n",
    "error_indices = dirty_data.index[dirty_data['error'] == 1]\n",
    "\n",
    "# 打印出这些行号\n",
    "print(error_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第二步：修正标记为 error == 1 的行（交换错误的纬度和经度）\n",
    "for idx, row in dirty_data.iterrows():\n",
    "    # 如果该行的 error 列标记为 1，则执行修正操作\n",
    "    if dirty_data.loc[idx, 'error'] == 1:\n",
    "        # 获取条件：纬度为正或经度为负\n",
    "        condition = (dirty_data.loc[idx, 'customer_long'] < 0) or (dirty_data.loc[idx, 'customer_lat'] > 0)\n",
    "        \n",
    "        if condition:\n",
    "            # 交换 customer_lat 和 customer_long 的值\n",
    "            temp_lat = dirty_data.loc[idx, 'customer_lat']\n",
    "            dirty_data.loc[idx, 'customer_lat'] = dirty_data.loc[idx, 'customer_long']\n",
    "            dirty_data.loc[idx, 'customer_long'] = temp_lat\n",
    "            \n",
    "            # 将该行的 error 更新为 2，表示已修正\n",
    "            dirty_data.loc[idx, 'error'] = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.4. Fix the nearest_warehouse<a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thompson     187\n",
      "Nickolson    186\n",
      "Bakers       108\n",
      "bakers         9\n",
      "thompson       7\n",
      "nickolson      3\n",
      "Name: nearest_warehouse, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "warehouse_counts = dirty_data['nearest_warehouse'].value_counts()\n",
    "print(warehouse_counts)\n",
    "\n",
    "for idx, row in dirty_data.iterrows():\n",
    "    # 如果 error 列标记为 0，检查 nearest_warehouse\n",
    "    if dirty_data.loc[idx, 'error'] == 0:\n",
    "        # 如果 nearest_warehouse 不在指定的三个仓库列表中，标记为 error = 1\n",
    "        if dirty_data.loc[idx, 'nearest_warehouse'] not in [\"Nickolson\", \"Thompson\", \"Bakers\"]:\n",
    "            dirty_data.loc[idx, 'error'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历 dirty_data 中的每一行\n",
    "for idx, row in dirty_data.iterrows():\n",
    "    # 如果 error 列标记为 1，检查 nearest_warehouse\n",
    "    if dirty_data.loc[idx, 'error'] == 1:\n",
    "        # 获取当前 nearest_warehouse 的值\n",
    "        warehouse = dirty_data.loc[idx, 'nearest_warehouse'].lower()  # 将其转为小写，便于比较\n",
    "        \n",
    "        # 如果 warehouse 是 \"bakers\"、\"thompson\" 或 \"nickolson\"，则将其转为首字母大写\n",
    "        if warehouse in [\"bakers\", \"thompson\", \"nickolson\"]:\n",
    "            dirty_data.loc[idx, 'nearest_warehouse'] = warehouse.capitalize()  # 首字母大写\n",
    "            dirty_data.loc[idx, 'error'] = 2  # 将 error 列标记为 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.5. Fix the distance_to_nearest_warehouse<a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取仓库数据\n",
    "warehouse = pd.read_csv(\"warehouses.csv\")\n",
    "\n",
    "# 创建字典，将仓库名称与对应的纬度和经度配对\n",
    "lat = dict(zip(warehouse['names'], warehouse['lat']))\n",
    "lon = dict(zip(warehouse['names'], warehouse['lon']))\n",
    "\n",
    "# 定义哈弗赛因公式计算两点之间的距离\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    dLat = (lat2 - lat1) * math.pi / 180.0\n",
    "    dLon = (lon2 - lon1) * math.pi / 180.0\n",
    "    lat1 = lat1 * math.pi / 180.0\n",
    "    lat2 = lat2 * math.pi / 180.0\n",
    "\n",
    "    a = (pow(math.sin(dLat / 2), 2) + \n",
    "         pow(math.sin(dLon / 2), 2) * \n",
    "         math.cos(lat1) * math.cos(lat2))\n",
    "\n",
    "    # 地球半径（单位：公里）\n",
    "    rad = 6378\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return rad * c\n",
    "\n",
    "# 初始化 distance_computed 列\n",
    "dirty_data['distance_computed'] = None\n",
    "\n",
    "# 遍历每个客户的记录，计算到 nearest_warehouse 的距离\n",
    "for index, row in dirty_data.iterrows():\n",
    "    # 获取客户的纬度和经度\n",
    "    customer_lat = row['customer_lat']\n",
    "    customer_long = row['customer_long']\n",
    "    \n",
    "    # 获取 nearest_warehouse 名称\n",
    "    nearest_warehouse = row['nearest_warehouse']\n",
    "    \n",
    "    # 检查 nearest_warehouse 是否在字典中\n",
    "    if nearest_warehouse in lat and nearest_warehouse in lon:\n",
    "        # 计算客户与 nearest_warehouse 的距离\n",
    "        dist = round(haversine(customer_lat, customer_long, lat[nearest_warehouse], lon[nearest_warehouse]), 4)\n",
    "        \n",
    "        # 将计算的距离存入 distance_computed 列\n",
    "        dirty_data.loc[index, 'distance_computed'] = dist\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以下行的距离值不相等，需要修正：[5, 18, 19, 29, 34, 39, 76, 78, 88, 96, 99, 112, 124, 143, 162, 165, 174, 207, 254, 274, 279, 311, 324, 346, 352, 356, 381, 385, 389, 396, 400, 401, 439, 440, 484]\n",
      "     distance_to_nearest_warehouse  error\n",
      "5                           3.4051      2\n",
      "18                          1.3150      2\n",
      "19                          0.7133      2\n",
      "29                          0.7403      2\n",
      "34                          1.5612      2\n",
      "39                          0.9221      2\n",
      "76                          2.1753      2\n",
      "78                          5.5590      2\n",
      "88                          0.7450      2\n",
      "96                          0.9259      2\n",
      "99                          2.1706      2\n",
      "112                         5.0801      2\n",
      "124                         0.9636      2\n",
      "143                         0.6619      2\n",
      "162                         1.5315      2\n",
      "165                         1.7155      2\n",
      "174                         0.9547      2\n",
      "207                         0.8881      2\n",
      "254                         0.8451      2\n",
      "274                         5.9795      2\n",
      "279                         0.4666      2\n",
      "311                         3.7330      2\n",
      "324                         2.6317      2\n",
      "346                         1.3841      2\n",
      "352                         5.1981      2\n",
      "356                         1.1811      2\n",
      "381                         0.8840      2\n",
      "385                         1.1434      2\n",
      "389                         1.8563      2\n",
      "396                         0.9333      2\n",
      "400                         1.5298      2\n",
      "401                         1.4938      2\n",
      "439                         0.8925      2\n",
      "440                         0.7896      2\n",
      "484                         1.2881      2\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个列表来记录不相等的行的行号\n",
    "mismatch_indices = []\n",
    "\n",
    "# 遍历 dirty_data 的每一行\n",
    "for idx, row in dirty_data.iterrows():\n",
    "    # 只对 error 为 0 的行进行操作\n",
    "    if dirty_data.loc[idx, 'error'] == 0:\n",
    "        # 获取原始的 distance_to_nearest_warehouse 和 新计算的 distance_computed\n",
    "        original_distance = dirty_data.loc[idx, 'distance_to_nearest_warehouse']\n",
    "        computed_distance = dirty_data.loc[idx, 'distance_computed']\n",
    "        \n",
    "        # 如果两者不相等\n",
    "        if original_distance != computed_distance:\n",
    "            mismatch_indices.append(idx)\n",
    "            dirty_data.loc[idx, 'distance_to_nearest_warehouse'] = computed_distance\n",
    "            dirty_data.loc[idx, 'error'] = 2\n",
    "\n",
    "# 打印记录不相等的行号\n",
    "print(f\"以下行的距离值不相等，需要修正：{mismatch_indices}\")\n",
    "\n",
    "# 删除 distance_computed 列\n",
    "dirty_data.drop(columns=['distance_computed'], inplace=True)\n",
    "\n",
    "# 打印修正后的数据进行检查\n",
    "print(dirty_data.loc[mismatch_indices, ['distance_to_nearest_warehouse', 'error']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([  0,   1,   2,   3,   4,   6,   7,   8,   9,  10,\n",
      "            ...\n",
      "            488, 489, 490, 491, 492, 494, 496, 497, 498, 499],\n",
      "           dtype='int64', length=392)\n"
     ]
    }
   ],
   "source": [
    "# 获取 dirty_data 中 error 列标记为 1 的行号\n",
    "error_indices = dirty_data.index[dirty_data['error'] == 0]\n",
    "\n",
    "# 打印出这些行号\n",
    "print(error_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.6. Fix the order_total<a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iAssist Line', 'Lucent 330S', 'Toshika 750', 'Thunder line', 'Olivia x460', 'Universe Note', 'iStream', 'Alcon 10', 'Candle Inferno', 'pearTV']\n",
      "Brand Prices (Rounded):\n",
      "Brand: iAssist Line, Price: 2225\n",
      "Brand: Lucent 330S, Price: 1230\n",
      "Brand: Toshika 750, Price: 4320\n",
      "Brand: Thunder line, Price: 2180\n",
      "Brand: Olivia x460, Price: 1225\n",
      "Brand: Universe Note, Price: 3450\n",
      "Brand: iStream, Price: 150\n",
      "Brand: Alcon 10, Price: 8950\n",
      "Brand: Candle Inferno, Price: 430\n",
      "Brand: pearTV, Price: 6310\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个 Counter 来统计每个品牌的销售数量\n",
    "brand_counter = Counter()\n",
    "\n",
    "# 遍历 outlier_data 数据集以获取所有品牌\n",
    "for index, row in outlier_data.iterrows():\n",
    "    # 将 shopping_cart 列的字符串转换为 Python 列表\n",
    "    shopping_cart = ast.literal_eval(row['shopping_cart'])\n",
    "    # 遍历购物车中的每个商品\n",
    "    for item in shopping_cart:\n",
    "        brand_name = item[0]  # 获取商品的品牌名称\n",
    "        brand_counter[brand_name] += 1  # 统计每个品牌的出现次数\n",
    "\n",
    "# 获取所有的品牌列表\n",
    "item_types = list(brand_counter.keys())\n",
    "print(item_types)\n",
    "\n",
    "# 初始化矩阵 A 和向量 b\n",
    "A = np.zeros((len(outlier_data), len(item_types)))  # A 矩阵的形状是 (订单数量, 品牌数量)\n",
    "b = np.zeros(len(outlier_data))  # b 是存储订单价格的向量\n",
    "\n",
    "# 遍历 outlier_data 数据集来填充矩阵 A 和向量 b\n",
    "for index, row in outlier_data.iterrows():\n",
    "    # 将 shopping_cart 列的字符串形式转换为 Python 列表\n",
    "    shopping_cart = ast.literal_eval(row[\"shopping_cart\"])  # 使用 ast.literal_eval 提高安全性\n",
    "    # 将订单的价格存储到向量 b 中\n",
    "    b[index] = row[\"order_price\"]\n",
    "    # 遍历购物车中的每个商品和数量\n",
    "    for item in shopping_cart:\n",
    "        brand_name = item[0]\n",
    "        quantity = item[1]\n",
    "        # 如果商品属于已知的 item_types\n",
    "        if brand_name in item_types:\n",
    "            # 找到该商品在 item_types 列表中的索引\n",
    "            item_index = item_types.index(brand_name)\n",
    "            # 将商品的数量添加到矩阵 A 的相应位置\n",
    "            A[index, item_index] += quantity\n",
    "\n",
    "# 检查 A 和 b 中是否有全为零的行或无效值\n",
    "valid_indices = np.where(A.any(axis=1) & ~np.isnan(b))[0]\n",
    "A = A[valid_indices]\n",
    "b = b[valid_indices]\n",
    "\n",
    "# 使用 np.linalg.lstsq() 计算每个品牌的价格\n",
    "prices, residuals, rank, s = np.linalg.lstsq(A, b, rcond=None)\n",
    "\n",
    "# 对价格进行四舍五入处理，不保留小数点\n",
    "rounded_prices = np.round(prices)\n",
    "\n",
    "# 将结果转换为品牌价格的字典\n",
    "price_dict = dict(zip(item_types, rounded_prices))\n",
    "\n",
    "# 输出每个品牌的价格\n",
    "print(\"Brand Prices (Rounded):\")\n",
    "for brand, price in price_dict.items():\n",
    "    print(f\"Brand: {brand}, Price: {int(price)}\")  # 转换为整数形式输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_data['order_computed'] = None\n",
    "# 遍历每一行数据\n",
    "for idx, row in dirty_data.iterrows():\n",
    "    # 将 shopping_cart 列的字符串转换为 Python 列表\n",
    "    shopping_cart = ast.literal_eval(row['shopping_cart'])\n",
    "    # 如果 error 列为 0，则计算购物车的总价\n",
    "    if dirty_data.loc[idx, 'error'] == 0:\n",
    "        total_price = 0\n",
    "        # 遍历购物车中的每个商品\n",
    "        for item in shopping_cart:\n",
    "            brand, quantity = item  # 每个 item 是 (brand, quantity) 的形式\n",
    "            # 计算当前商品的总价\n",
    "            if brand in price_dict:\n",
    "                total_price += price_dict[brand] * quantity\n",
    "        # 将计算出来的总价存入 order_computed 列\n",
    "        dirty_data.loc[idx, 'order_computed'] = total_price\n",
    "    else:\n",
    "        # 如果 error 不为 0，则将 order_computed 设置为 order_price\n",
    "        dirty_data.loc[idx, 'order_computed'] = row['order_price']\n",
    "# print(dirty_data['order_computed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在第 4 行发现匹配成功，替换后的购物车：[('Thunder line', 1), ('Universe Note', 1), ('iAssist Line', 2)]\n",
      "在第 11 行发现匹配成功，替换后的购物车：[('Thunder line', 1), ('Olivia x460', 2), ('Candle Inferno', 1)]\n",
      "在第 23 行发现匹配成功，替换后的购物车：[('Universe Note', 2), ('Candle Inferno', 2)]\n",
      "在第 30 行发现匹配成功，替换后的购物车：[('iAssist Line', 1), ('Lucent 330S', 1), ('Universe Note', 1), ('Alcon 10', 1)]\n",
      "在第 44 行发现匹配成功，替换后的购物车：[('iAssist Line', 1), ('Lucent 330S', 1), ('Toshika 750', 1), ('pearTV', 2)]\n",
      "在第 56 行发现匹配成功，替换后的购物车：[('Toshika 750', 2), ('Alcon 10', 2), ('Candle Inferno', 2)]\n",
      "在第 98 行发现匹配成功，替换后的购物车：[('Thunder line', 1), ('Alcon 10', 1), ('iAssist Line', 2)]\n",
      "在第 130 行发现匹配成功，替换后的购物车：[('Toshika 750', 2), ('Olivia x460', 2), ('Universe Note', 1), ('iStream', 1)]\n",
      "在第 173 行发现匹配成功，替换后的购物车：[('Toshika 750', 1), ('Alcon 10', 1), ('Candle Inferno', 1), ('Thunder line', 2)]\n",
      "在第 181 行发现匹配成功，替换后的购物车：[('iAssist Line', 1), ('iStream', 1), ('Thunder line', 2), ('Candle Inferno', 1)]\n",
      "在第 189 行发现匹配成功，替换后的购物车：[('Toshika 750', 1), ('Universe Note', 2)]\n",
      "在第 206 行发现匹配成功，替换后的购物车：[('Lucent 330S', 2), ('Universe Note', 2)]\n",
      "在第 215 行发现匹配成功，替换后的购物车：[('iAssist Line', 2), ('Toshika 750', 2), ('Thunder line', 2), ('Alcon 10', 1)]\n",
      "在第 228 行发现匹配成功，替换后的购物车：[('Thunder line', 2), ('Universe Note', 2)]\n",
      "在第 234 行发现匹配成功，替换后的购物车：[('iAssist Line', 1), ('Candle Inferno', 1), ('iStream', 2), ('pearTV', 2)]\n",
      "在第 252 行发现匹配成功，替换后的购物车：[('iAssist Line', 2), ('Alcon 10', 1), ('Candle Inferno', 2)]\n",
      "在第 256 行发现匹配成功，替换后的购物车：[('iAssist Line', 2), ('Thunder line', 2), ('Olivia x460', 2)]\n",
      "在第 259 行发现匹配成功，替换后的购物车：[('iAssist Line', 1), ('iStream', 1), ('Alcon 10', 1)]\n",
      "在第 296 行发现匹配成功，替换后的购物车：[('Thunder line', 2), ('Olivia x460', 2)]\n",
      "在第 303 行发现匹配成功，替换后的购物车：[('Toshika 750', 2), ('Thunder line', 1), ('Universe Note', 1)]\n",
      "在第 318 行发现匹配成功，替换后的购物车：[('pearTV', 1), ('Candle Inferno', 2)]\n",
      "在第 336 行发现匹配成功，替换后的购物车：[('Toshika 750', 1), ('iStream', 1), ('pearTV', 2), ('Alcon 10', 1)]\n",
      "在第 359 行发现匹配成功，替换后的购物车：[('Toshika 750', 2), ('Alcon 10', 2)]\n",
      "在第 368 行发现匹配成功，替换后的购物车：[('Universe Note', 2), ('iStream', 2), ('iAssist Line', 1)]\n",
      "在第 370 行发现匹配成功，替换后的购物车：[('Lucent 330S', 1), ('iStream', 1)]\n",
      "在第 372 行发现匹配成功，替换后的购物车：[('iAssist Line', 1), ('iStream', 1), ('Alcon 10', 2), ('Candle Inferno', 2)]\n",
      "在第 374 行发现匹配成功，替换后的购物车：[('Toshika 750', 1), ('Olivia x460', 2)]\n",
      "在第 394 行发现匹配成功，替换后的购物车：[('Candle Inferno', 1), ('Toshika 750', 2)]\n",
      "在第 405 行发现匹配成功，替换后的购物车：[('Universe Note', 2), ('pearTV', 2)]\n",
      "在第 416 行发现匹配成功，替换后的购物车：[('Thunder line', 1), ('Candle Inferno', 1), ('Universe Note', 2)]\n",
      "在第 421 行发现匹配成功，替换后的购物车：[('iAssist Line', 2), ('Lucent 330S', 2)]\n",
      "在第 489 行发现匹配成功，替换后的购物车：[('Toshika 750', 2), ('Alcon 10', 1)]\n",
      "在第 498 行发现匹配成功，替换后的购物车：[('Lucent 330S', 2), ('Olivia x460', 1), ('Candle Inferno', 1)]\n",
      "匹配成功的行号: [4, 11, 23, 30, 44, 56, 98, 130, 173, 181, 189, 206, 215, 228, 234, 252, 256, 259, 296, 303, 318, 336, 359, 368, 370, 372, 374, 394, 405, 416, 421, 489, 498]\n",
      "匹配成功的行数: 33\n"
     ]
    }
   ],
   "source": [
    "mismatch_rows = dirty_data[dirty_data['order_computed'] != dirty_data['order_price']]\n",
    "mismatch_indices = mismatch_rows.index.tolist()\n",
    "for idx in mismatch_indices:\n",
    "    if dirty_data.loc[idx, 'error'] == 0:\n",
    "        dirty_data.loc[idx, 'error'] = 1\n",
    "\n",
    "import itertools\n",
    "# 初始化一个列表来存储匹配成功的行号\n",
    "successful_matches = []\n",
    "\n",
    "# 遍历所有 error == 1 的行\n",
    "for idx, row in dirty_data[dirty_data['error'] == 1].iterrows():\n",
    "    shopping_cart = ast.literal_eval(row['shopping_cart'])\n",
    "    original_order_price = row['order_price']\n",
    "\n",
    "    # 获取购物车中商品的数量，保持数量不变\n",
    "    quantities = [item[1] for item in shopping_cart]  # 只取数量，不替换品牌\n",
    "\n",
    "    # 获取所有品牌的排列组合，确保每个品牌只出现一次\n",
    "    if len(quantities) <= len(price_dict):  # 确保品牌数量足够\n",
    "        brand_combinations = itertools.permutations(price_dict.keys(), len(quantities))\n",
    "\n",
    "        # 遍历所有可能的品牌组合\n",
    "        for possible_combination in brand_combinations:\n",
    "            temp_total = 0\n",
    "            temp_shopping_cart = []\n",
    "\n",
    "            # 遍历购物车中的数量，使用不同品牌组合\n",
    "            for brand, quantity in zip(possible_combination, quantities):\n",
    "                temp_total += price_dict[brand] * quantity\n",
    "                temp_shopping_cart.append((brand, quantity))\n",
    "\n",
    "            # 如果替换后的购物车价格与原始 order_price 匹配（允许误差在10以内）\n",
    "            if abs(temp_total - original_order_price) <= 5:\n",
    "                print(f\"在第 {idx} 行发现匹配成功，替换后的购物车：{temp_shopping_cart}\")\n",
    "                \n",
    "                # 更新 dirty_data 中的 shopping_cart 和 order_computed\n",
    "                dirty_data.loc[idx, 'shopping_cart'] = str(temp_shopping_cart)  # 将更新后的购物车存储为字符串\n",
    "                dirty_data.loc[idx, 'order_computed'] = temp_total\n",
    "                dirty_data.loc[idx, 'error'] = 2  # 匹配成功，标记 error 为 3\n",
    "                \n",
    "                # 记录匹配成功的行号\n",
    "                successful_matches.append(idx)\n",
    "                break  # 一旦找到匹配，跳出品牌组合的循环\n",
    "\n",
    "# 输出匹配成功的行号和数量\n",
    "print(f\"匹配成功的行号: {successful_matches}\")\n",
    "print(f\"匹配成功的行数: {len(successful_matches)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共替换了 20 行数据的 order_price\n",
      "     order_price  error\n",
      "0           8130      0\n",
      "1           2750      0\n",
      "2           6820      0\n",
      "3           8555      0\n",
      "4          10080      2\n",
      "..           ...    ...\n",
      "495         6735      2\n",
      "496        26035      0\n",
      "497        30475      0\n",
      "498         4110      2\n",
      "499        21570      2\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个计数器\n",
    "replace_count = 0\n",
    "\n",
    "# 遍历所有 error == 1 的行\n",
    "for idx, row in dirty_data[dirty_data['error'] == 1].iterrows():\n",
    "    # 将 order_computed 的值替换到 order_price 上\n",
    "    dirty_data.loc[idx, 'order_price'] = dirty_data.loc[idx, 'order_computed']\n",
    "    \n",
    "    # 将 error 设置为 2\n",
    "    dirty_data.loc[idx, 'error'] = 2\n",
    "    \n",
    "    # 每次替换成功后计数器加1\n",
    "    replace_count += 1\n",
    "\n",
    "# 删除 order_computed 列\n",
    "dirty_data.drop(columns=['order_computed'], inplace=True)\n",
    "\n",
    "# 输出替换了多少行数据\n",
    "print(f\"共替换了 {replace_count} 行数据的 order_price\")\n",
    "\n",
    "# 检查替换和删除是否成功\n",
    "print(dirty_data[['order_price', 'error']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化 total_computed 列\n",
    "dirty_data['total_computed'] = None\n",
    "# 遍历每一行数据\n",
    "for idx, row in dirty_data.iterrows():\n",
    "    order_price = row['order_price']\n",
    "    \n",
    "    # 如果 error 列为 0，计算总价\n",
    "    if dirty_data.loc[idx, 'error'] == 0:\n",
    "        delivery_charges = row['delivery_charges']\n",
    "        coupon_discount = row['coupon_discount'] / 100  # 将百分比折扣转换为小数\n",
    "        # 计算总价：order_price 先应用折扣，再加上运费\n",
    "        total_computed = order_price * (1 - coupon_discount) + delivery_charges\n",
    "        # 将计算的总价存入 total_computed 列\n",
    "        dirty_data.loc[idx, 'total_computed'] = total_computed\n",
    "    else:\n",
    "        # 如果 error 不为 0，则将 total_computed 设置为 order_total\n",
    "        dirty_data.loc[idx, 'total_computed'] = row['order_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([  7,  13,  25,  27,  41,  71, 103, 106, 111, 118, 141, 157, 179,\n",
      "            186, 268, 289, 300, 316, 357, 358, 379, 404, 410, 448, 449, 481,\n",
      "            487],\n",
      "           dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "mismatch_rows = dirty_data[dirty_data['total_computed'] != dirty_data['order_total']]\n",
    "mismatch_indices = mismatch_rows.index.tolist()\n",
    "for idx in mismatch_indices:\n",
    "    if dirty_data.loc[idx, 'error'] == 0:\n",
    "        dirty_data.loc[idx, 'error'] = 1\n",
    "        \n",
    "# 获取 dirty_data 中 error 列标记为 1 的行号\n",
    "error_indices = dirty_data.index[dirty_data['error'] == 1]\n",
    "# 打印出这些行号\n",
    "print(error_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共替换了 27 行数据的 order_total\n",
      "     order_total  error\n",
      "0        7415.05      0\n",
      "1        2555.36      0\n",
      "2        5878.29      0\n",
      "3        6499.12      0\n",
      "4       10148.21      2\n",
      "..           ...    ...\n",
      "495      5135.42      2\n",
      "496     23502.94      0\n",
      "497     29017.60      0\n",
      "498      3951.68      2\n",
      "499     21643.81      2\n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 初始化计数器\n",
    "rreplace_count = 0\n",
    "\n",
    "# 遍历所有 error 值为 1 的行\n",
    "for idx, row in dirty_data[dirty_data['error'] == 1].iterrows():\n",
    "  \n",
    "    # 将 total_computed 的值替换到 order_total 上\n",
    "    dirty_data.loc[idx, 'order_total'] = dirty_data.loc[idx, 'total_computed']\n",
    "    \n",
    "    # 将 error 设置为 2\n",
    "    dirty_data.loc[idx, 'error'] = 2\n",
    "    \n",
    "    # 每次替换成功后计数器加1\n",
    "    rreplace_count += 1\n",
    "\n",
    "# 删除 total_computed 列\n",
    "dirty_data.drop(columns=['total_computed'], inplace=True)\n",
    "\n",
    "# 输出替换了多少行数据\n",
    "print(f\"共替换了 {rreplace_count} 行数据的 order_total\")\n",
    "\n",
    "# 检查替换和删除是否成功\n",
    "print(dirty_data[['order_total', 'error']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.10. Fix the is_happy_customer<a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# 确保下载 vader_lexicon\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# 初始化 SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# 确保 latest_customer_review 列为字符串类型\n",
    "dirty_data['latest_customer_review'] = dirty_data['latest_customer_review'].astype(str)\n",
    "\n",
    "# 定义一个函数来计算情绪得分\n",
    "def compute_sentiment(review_text):\n",
    "    # 如果评论是空字符串或缺失值，默认返回 True\n",
    "    if not review_text or review_text.strip() == \"None\":\n",
    "        return True\n",
    "    polarity_score = sia.polarity_scores(review_text)['compound']\n",
    "    return polarity_score >= 0.05  # 返回 True 表示积极情绪，False 表示消极情绪\n",
    "\n",
    "# 使用 apply 函数进行情绪分析，并将结果存入 test_sentiment 列\n",
    "dirty_data['test_sentiment'] = dirty_data['latest_customer_review'].apply(compute_sentiment)\n",
    "\n",
    "# 打印前几行结果以进行检查\n",
    "print(dirty_data[['latest_customer_review', 'test_sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "修改过 is_happy_customer 的行数: 0\n",
      "修改过的行号: []\n"
     ]
    }
   ],
   "source": [
    "# 初始化一个列表来存储修改过的行号\n",
    "modified_indices = []\n",
    "\n",
    "# 遍历所有 error == 0 的行\n",
    "for idx, row in dirty_data[dirty_data['error'] == 0].iterrows():\n",
    "    # 如果 test_sentiment 不等于 is_happy_customer\n",
    "    if row['test_sentiment'] != row['is_happy_customer']:\n",
    "        # 将 test_sentiment 的值赋给 is_happy_customer\n",
    "        dirty_data.loc[idx, 'is_happy_customer'] = row['test_sentiment']\n",
    "        \n",
    "        # 将 error 列标记为 2\n",
    "        dirty_data.loc[idx, 'error'] = 2\n",
    "        \n",
    "        # 记录修改过的行号\n",
    "        modified_indices.append(idx)\n",
    "\n",
    "# 打印修改过的行数和行号\n",
    "print(f\"修改过 is_happy_customer 的行数: {len(modified_indices)}\")\n",
    "print(f\"修改过的行号: {modified_indices}\")\n",
    "dirty_data.drop(columns=['test_sentiment'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.11. Fix the is_expedited_delivery<a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spring 模型系数: [0.0443219  0.09064435 0.99640716]\n",
      "Spring 模型截距: 0.7729074802802245\n",
      "Spring 均方误差: 0.11672739159582231\n",
      "Spring R^2 值: 0.9994893205299628\n",
      "==================================================\n",
      "Summer 模型系数: [-0.04765955 -0.2089709   1.00441788]\n",
      "Summer 模型截距: 0.24917508158780777\n",
      "Summer 均方误差: 0.09475318126791515\n",
      "Summer R^2 值: 0.9994946574110137\n",
      "==================================================\n",
      "Autumn 模型系数: [ 0.01651381 -0.0550806   1.00799579]\n",
      "Autumn 模型截距: -0.11635017972062656\n",
      "Autumn 均方误差: 0.09653793045460139\n",
      "Autumn R^2 值: 0.9990401882816209\n",
      "==================================================\n",
      "Winter 模型系数: [-0.01032114  0.08726834  0.99555374]\n",
      "Winter 模型截距: 0.7776426571535637\n",
      "Winter 均方误差: 0.08407617718869072\n",
      "Winter R^2 值: 0.9988974177835975\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# 假设 dirty_data 包含以下列：distance_to_nearest_warehouse, is_expedited_delivery, is_happy_customer, delivery_charges, season\n",
    "dirty_data['transfer_is_expedited_delivery'] = dirty_data['is_expedited_delivery'].astype(int)\n",
    "dirty_data['transfer_delivery_charges'] = dirty_data['delivery_charges'].astype(int)\n",
    "# 1. Spring (春季)\n",
    "spring_data = dirty_data[dirty_data['season'] == 'Spring']\n",
    "X_spring = spring_data[['distance_to_nearest_warehouse', 'transfer_is_expedited_delivery', 'transfer_delivery_charges']]\n",
    "y_spring = spring_data['delivery_charges']\n",
    "\n",
    "X_train_spring, X_test_spring, y_train_spring, y_test_spring = train_test_split(X_spring, y_spring, test_size=0.2, random_state=42)\n",
    "lm_for_spring = LinearRegression()\n",
    "lm_for_spring.fit(X_train_spring, y_train_spring)\n",
    "\n",
    "# 评估 Spring 模型\n",
    "y_pred_spring = lm_for_spring.predict(X_test_spring)\n",
    "mse_spring = mean_squared_error(y_test_spring, y_pred_spring)\n",
    "r2_spring = r2_score(y_test_spring, y_pred_spring)\n",
    "\n",
    "print(f\"Spring 模型系数: {lm_for_spring.coef_}\")\n",
    "print(f\"Spring 模型截距: {lm_for_spring.intercept_}\")\n",
    "print(f\"Spring 均方误差: {mse_spring}\")\n",
    "print(f\"Spring R^2 值: {r2_spring}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 2. Summer (夏季)\n",
    "summer_data = dirty_data[dirty_data['season'] == 'Summer']\n",
    "X_summer = summer_data[['distance_to_nearest_warehouse', 'transfer_is_expedited_delivery', 'transfer_delivery_charges']]\n",
    "y_summer = summer_data['delivery_charges']\n",
    "\n",
    "X_train_summer, X_test_summer, y_train_summer, y_test_summer = train_test_split(X_summer, y_summer, test_size=0.2, random_state=42)\n",
    "lm_for_summer = LinearRegression()\n",
    "lm_for_summer.fit(X_train_summer, y_train_summer)\n",
    "\n",
    "# 评估 Summer 模型\n",
    "y_pred_summer = lm_for_summer.predict(X_test_summer)\n",
    "mse_summer = mean_squared_error(y_test_summer, y_pred_summer)\n",
    "r2_summer = r2_score(y_test_summer, y_pred_summer)\n",
    "\n",
    "print(f\"Summer 模型系数: {lm_for_summer.coef_}\")\n",
    "print(f\"Summer 模型截距: {lm_for_summer.intercept_}\")\n",
    "print(f\"Summer 均方误差: {mse_summer}\")\n",
    "print(f\"Summer R^2 值: {r2_summer}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 3. Autumn (秋季)\n",
    "autumn_data = dirty_data[dirty_data['season'] == 'Autumn']\n",
    "X_autumn = autumn_data[['distance_to_nearest_warehouse', 'transfer_is_expedited_delivery', 'transfer_delivery_charges']]\n",
    "y_autumn = autumn_data['delivery_charges']\n",
    "\n",
    "X_train_autumn, X_test_autumn, y_train_autumn, y_test_autumn = train_test_split(X_autumn, y_autumn, test_size=0.2, random_state=42)\n",
    "lm_for_autumn = LinearRegression()\n",
    "lm_for_autumn.fit(X_train_autumn, y_train_autumn)\n",
    "\n",
    "# 评估 Autumn 模型\n",
    "y_pred_autumn = lm_for_autumn.predict(X_test_autumn)\n",
    "mse_autumn = mean_squared_error(y_test_autumn, y_pred_autumn)\n",
    "r2_autumn = r2_score(y_test_autumn, y_pred_autumn)\n",
    "\n",
    "print(f\"Autumn 模型系数: {lm_for_autumn.coef_}\")\n",
    "print(f\"Autumn 模型截距: {lm_for_autumn.intercept_}\")\n",
    "print(f\"Autumn 均方误差: {mse_autumn}\")\n",
    "print(f\"Autumn R^2 值: {r2_autumn}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 4. Winter (冬季)\n",
    "winter_data = dirty_data[dirty_data['season'] == 'Winter']\n",
    "X_winter = winter_data[['distance_to_nearest_warehouse', 'transfer_is_expedited_delivery', 'transfer_delivery_charges']]\n",
    "y_winter = winter_data['delivery_charges']\n",
    "\n",
    "X_train_winter, X_test_winter, y_train_winter, y_test_winter = train_test_split(X_winter, y_winter, test_size=0.2, random_state=42)\n",
    "lm_for_winter = LinearRegression()\n",
    "lm_for_winter.fit(X_train_winter, y_train_winter)\n",
    "\n",
    "# 评估 Winter 模型\n",
    "y_pred_winter = lm_for_winter.predict(X_test_winter)\n",
    "mse_winter = mean_squared_error(y_test_winter, y_pred_winter)\n",
    "r2_winter = r2_score(y_test_winter, y_pred_winter)\n",
    "\n",
    "print(f\"Winter 模型系数: {lm_for_winter.coef_}\")\n",
    "print(f\"Winter 模型截距: {lm_for_winter.intercept_}\")\n",
    "print(f\"Winter 均方误差: {mse_winter}\")\n",
    "print(f\"Winter R^2 值: {r2_winter}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 5.  Detect and remove outlier rows <a class=\"anchor\" name=\"load\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "检测出的离群值有 3 行数据\n",
      "     delivery_charges\n",
      "276           137.670\n",
      "330           145.995\n",
      "472           131.070\n",
      "移除后剩余的行数: 497\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 计算 delivery_charges 的均值和标准差\n",
    "mean_delivery_charges = outlier_data['delivery_charges'].mean()\n",
    "std_delivery_charges = outlier_data['delivery_charges'].std()\n",
    "\n",
    "# 使用 3 Sigma 法则确定上下界\n",
    "lower_bound = mean_delivery_charges - 3 * std_delivery_charges\n",
    "upper_bound = mean_delivery_charges + 3 * std_delivery_charges\n",
    "\n",
    "# 识别离群点（低于下界或高于上界）\n",
    "outliers = outlier_data[(outlier_data['delivery_charges'] < lower_bound) | (outlier_data['delivery_charges'] > upper_bound)]\n",
    "\n",
    "# 打印检测出的离群值\n",
    "print(f\"检测出的离群值有 {len(outliers)} 行数据\")\n",
    "print(outliers[['delivery_charges']])\n",
    "\n",
    "# 从原始数据中移除离群点的行\n",
    "cleaned_data = outlier_data.drop(outliers.index)\n",
    "\n",
    "# 保存处理后的数据\n",
    "cleaned_data.to_csv('cleaned_outlier_data.csv', index=False)\n",
    "\n",
    "# 检查移除离群点后的数据\n",
    "print(f\"移除后剩余的行数: {len(cleaned_data)}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task1_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
