{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCbAmQ47iqK4"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# FIT5196 Task 1 in Assessment 2\n",
    "#### Student Name: Deshui Yu      Liangjing Yang\n",
    "#### Student ID: 34253599      34060871\n",
    "\n",
    "Date: 28/09/2024\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjBFqYK4iqK5"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>    \n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Patent Files](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Defining Regular Expressions](#Reg_Exp) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Reading Files](#Read) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Whatever else](#latin) <br>\n",
    "[5. Writing to CSV/JSON File](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Verification - using the sample files](#test_xml) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcbqK3KliqK6"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEFdSCIUiqK6"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project involves cleansing and analyzing a retail transactional dataset from DigiCO, an online electronics store in Melbourne. The task is to detect and fix errors, impute missing values, and remove outliers using exploratory data analysis (EDA). Cleaned data will be saved in the required output files, and the process will be documented in the final report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:\n",
    "\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** to manage and analyze data.\n",
    "* **datetime** to handle dates and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 3.  Examining Raw Data <a class=\"anchor\" name=\"examine\"></a>\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到这三个文件都包含以下数据列：order_id、customer_id、date、nearest_warehouse、shopping_cart、order_price、delivery_charges、customer_lat、customer_long、coupon_discount、order_total、season、is_expedited_delivery、distance_to_nearest_warehouse、latest_customer_review 和 is_happy_customer。其中，coupon_discount、delivery_charges、shopping_cart 中的商品数量、order_id、customer_id 和 latest_customer_review 是没有错误的数据。在 Group181_missing_data.csv 文件中，is_happy_customer 列的数据缺失，其数据类型为 float64；在 Group181_dirty_data.csv 中存在错误数据；而在 Group181_outlier_data.csv 文件中则有异常数据。\n",
    "通过数据逻辑我们可以发现，date和season数据是有关系的，customer_lat and customer_long 和 distance_to_nearest_warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 16 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   order_id                       500 non-null    object \n",
      " 1   customer_id                    500 non-null    object \n",
      " 2   date                           500 non-null    object \n",
      " 3   nearest_warehouse              500 non-null    object \n",
      " 4   shopping_cart                  500 non-null    object \n",
      " 5   order_price                    500 non-null    int64  \n",
      " 6   delivery_charges               500 non-null    float64\n",
      " 7   customer_lat                   500 non-null    float64\n",
      " 8   customer_long                  500 non-null    float64\n",
      " 9   coupon_discount                500 non-null    int64  \n",
      " 10  order_total                    500 non-null    float64\n",
      " 11  season                         500 non-null    object \n",
      " 12  is_expedited_delivery          500 non-null    bool   \n",
      " 13  distance_to_nearest_warehouse  500 non-null    float64\n",
      " 14  latest_customer_review         499 non-null    object \n",
      " 15  is_happy_customer              500 non-null    bool   \n",
      "dtypes: bool(2), float64(5), int64(2), object(7)\n",
      "memory usage: 55.8+ KB\n",
      "None\n",
      "        order_price  delivery_charges  customer_lat  customer_long  \\\n",
      "count    500.000000        500.000000    500.000000     500.000000   \n",
      "mean   13342.470000         75.918160    -27.942796     135.097474   \n",
      "std     7776.483817         14.202067     41.352716      41.352730   \n",
      "min      730.000000         47.180000    -37.828205     -37.822644   \n",
      "25%     7633.750000         65.637500    -37.818802     144.950075   \n",
      "50%    11325.000000         75.570000    -37.812816     144.964726   \n",
      "75%    18200.000000         82.110000    -37.805630     144.980696   \n",
      "max    42610.000000        125.110000    145.016109     145.017014   \n",
      "\n",
      "       coupon_discount   order_total  distance_to_nearest_warehouse  \n",
      "count       500.000000    500.000000                     500.000000  \n",
      "mean         11.110000  12190.423840                       1.056998  \n",
      "std           8.518393   7377.275081                       0.506665  \n",
      "min           0.000000   1061.310000                       0.057000  \n",
      "25%           5.000000   6627.600000                       0.702700  \n",
      "50%          10.000000  10380.555000                       1.018250  \n",
      "75%          15.000000  16883.550000                       1.380500  \n",
      "max          25.000000  42711.890000                       3.718800  \n"
     ]
    }
   ],
   "source": [
    "dirty_file_path = 'Group181_dirty_data.csv'\n",
    "dirty_data = pd.read_csv(dirty_file_path)\n",
    "print(dirty_data.info())\n",
    "print(dirty_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 16 columns):\n",
      " #   Column                         Non-Null Count  Dtype  \n",
      "---  ------                         --------------  -----  \n",
      " 0   order_id                       500 non-null    object \n",
      " 1   customer_id                    500 non-null    object \n",
      " 2   date                           500 non-null    object \n",
      " 3   nearest_warehouse              445 non-null    object \n",
      " 4   shopping_cart                  500 non-null    object \n",
      " 5   order_price                    485 non-null    float64\n",
      " 6   delivery_charges               460 non-null    float64\n",
      " 7   customer_lat                   500 non-null    float64\n",
      " 8   customer_long                  500 non-null    float64\n",
      " 9   coupon_discount                500 non-null    int64  \n",
      " 10  order_total                    485 non-null    float64\n",
      " 11  season                         500 non-null    object \n",
      " 12  is_expedited_delivery          500 non-null    bool   \n",
      " 13  distance_to_nearest_warehouse  469 non-null    float64\n",
      " 14  latest_customer_review         500 non-null    object \n",
      " 15  is_happy_customer              460 non-null    float64\n",
      "dtypes: bool(1), float64(7), int64(1), object(7)\n",
      "memory usage: 59.2+ KB\n",
      "None\n",
      "        order_price  delivery_charges  customer_lat  customer_long  \\\n",
      "count    485.000000        460.000000    500.000000     500.000000   \n",
      "mean   13955.144330         78.048239    -37.812175     144.965977   \n",
      "std     7912.200354         14.682956      0.007666       0.022431   \n",
      "min      730.000000         45.420000    -37.827409     144.917167   \n",
      "25%     8130.000000         66.272500    -37.818736     144.949193   \n",
      "50%    13220.000000         77.695000    -37.812649     144.962898   \n",
      "75%    18630.000000         85.602500    -37.805883     144.980895   \n",
      "max    39160.000000        113.480000    -37.792631     145.020271   \n",
      "\n",
      "       coupon_discount   order_total  distance_to_nearest_warehouse  \\\n",
      "count       500.000000    485.000000                     469.000000   \n",
      "mean         10.640000  12565.880474                       1.100080   \n",
      "std           8.150046   7179.953977                       0.520032   \n",
      "min           0.000000    611.590000                       0.050800   \n",
      "25%           5.000000   7055.260000                       0.741100   \n",
      "50%          10.000000  11735.520000                       1.059100   \n",
      "75%          15.000000  16956.140000                       1.438300   \n",
      "max          25.000000  35376.290000                       2.851300   \n",
      "\n",
      "       is_happy_customer  \n",
      "count         460.000000  \n",
      "mean            0.760870  \n",
      "std             0.427017  \n",
      "min             0.000000  \n",
      "25%             1.000000  \n",
      "50%             1.000000  \n",
      "75%             1.000000  \n",
      "max             1.000000  \n"
     ]
    }
   ],
   "source": [
    "missing_file_path = 'Group181_missing_data.csv'\n",
    "missing_data = pd.read_csv(missing_file_path)\n",
    "print(missing_data.info())\n",
    "print(missing_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 4.  Detect and fix errors in dirty_data <a class=\"anchor\" name=\"load\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.1. Fix the date and season <a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through examining the current data, the issue was identified as the date values, which were supposed to be in the YYYY-MM-DD format, mistakenly being formatted as YYYY-DD-MM and DD-MM-YYYY. Additionally, based on logical reasoning, the date and season are highly correlated, so after fixing the date data, the season data should also be corrected accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and mark invalid dates\n",
    "# reference from chatGPT\n",
    "# Temporarily convert the 'date' column to datetime format, marking invalid dates as NaT (Not a Time)\n",
    "temp_dates = pd.to_datetime(dirty_data['date'], errors='coerce')\n",
    "# Find the invalid date\n",
    "invalid_dates_temp = dirty_data[temp_dates.isna()]\n",
    "print(invalid_dates_temp[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_dates = []\n",
    "# Loop through each date string in the 'date' column\n",
    "for date_str in dirty_data['date']:\n",
    "    parts = date_str.split('-')\n",
    "    # Check if format is DD-MM-YYYY and fix to YYYY-MM-DD\n",
    "    if len(parts) == 3 and int(parts[0]) <= 12 and int(parts[1]) <= 31:\n",
    "        fixed_dates.append(f'{parts[2]}-{parts[1]}-{parts[0]}')  # DD-MM-YYYY -> YYYY-MM-DD\n",
    "    # Check if format is YYYY-DD-MM and fix to YYYY-MM-DD\n",
    "    elif len(parts) == 3 and int(parts[1]) > 12 and int(parts[2]) <= 12:\n",
    "        fixed_dates.append(f'{parts[0]}-{parts[2]}-{parts[1]}')  # YYYY-DD-MM -> YYYY-MM-DD\n",
    "    # Keep original format for other cases\n",
    "    else:\n",
    "        fixed_dates.append(date_str)\n",
    "# Convert the fixed dates back to datetime format and replace the original 'date' column\n",
    "dirty_data['date'] = pd.to_datetime(fixed_dates, errors='coerce')\n",
    "print(dirty_data['date'].type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the date and season data are logically related, once the date data has been corrected, the season data should also be updated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_seasons = []\n",
    "# Modify the season data based on the fixed date\n",
    "for date in dirty_data['date']:\n",
    "    month = date.month # Extract the month\n",
    "    if month in [9, 10, 11]:\n",
    "        fixed_seasons.append('Spring')\n",
    "    elif month in [12, 1, 2]:\n",
    "        fixed_seasons.append('Summer')\n",
    "    elif month in [3, 4, 5]:\n",
    "        fixed_seasons.append('Autumn')\n",
    "    elif month in [6, 7, 8]:\n",
    "        fixed_seasons.append('Winter')\n",
    "# Replace the original season column with the generated fixed_seasons\n",
    "dirty_data['season'] = fixed_seasons\n",
    "\n",
    "print(dirty_data[['date', 'season']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.2. Fix the customer_lat, customer_long, distance_to_nearest_warehouse and nearest_warehouse<a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this business is based in Melbourne, the correct values for latitude and longitude should be Latitude: -37.8136° and Longitude: 144.9631°. However, I have discovered some incorrect data where the latitude and longitude values were swapped. After correcting these errors, the distance_to_nearest_warehouse and nearest_warehouse fields, which are calculated based on the latitude and longitude, may also need to be fixed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for latitude greater than 0 (latitude in Australia should be less than 0)\n",
    "lat_issue = dirty_data.loc[dirty_data['customer_lat'] > 0]\n",
    "print(lat_issue[['customer_lat', 'customer_long']])\n",
    "\n",
    "# Check for longitude less than 0 (longitude in Australia should be greater than 0)\n",
    "long_issue = dirty_data.loc[dirty_data['customer_long'] < 0]\n",
    "print(long_issue[['customer_lat', 'customer_long']])\n",
    "\n",
    "# Condition to find rows where either longitude < 0 or latitude > 0\n",
    "condition = (dirty_data['customer_long'] < 0) | (dirty_data['customer_lat'] > 0)\n",
    "\n",
    "# Select the rows matching the condition and swap the latitude and longitude\n",
    "#reference from chatGPT\n",
    "dirty_data.loc[condition, ['customer_lat', 'customer_long']] = \\\n",
    "    dirty_data.loc[condition, ['customer_long', 'customer_lat']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我fix好了用户的经纬度后，我需要重新计算distance_to_nearest_warehouse，并且重新划分nearest_warehouse的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  nearest_warehouse  distance_to_nearest_warehouse\n",
      "0          Thompson                         1.2094\n",
      "1            Bakers                         1.2622\n",
      "2         Nickolson                         1.2569\n",
      "3          Thompson                         0.4238\n",
      "4          Thompson                         1.4394\n"
     ]
    }
   ],
   "source": [
    "# Load warehouse data\n",
    "warehouse = pd.read_csv(\"warehouses.csv\")\n",
    "lat = dict(zip(warehouse['names'], warehouse['lat']))\n",
    "lon = dict(zip(warehouse['names'], warehouse['lon']))\n",
    "\n",
    "# reference from https://stackoverflow.com/questions/4913349/haversine-formula-in-python-bearing-and-distance-between-two-gps-points\n",
    "# Haversine function to calculate distance between two coordinates\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    dLat = (lat2 - lat1) * math.pi / 180.0 \n",
    "    dLon = (lon2 - lon1) * math.pi / 180.0\n",
    "    lat1 = (lat1) * math.pi / 180.0\n",
    "    lat2 = (lat2) * math.pi / 180.0\n",
    "    \n",
    "    a = (pow(math.sin(dLat / 2), 2) + \n",
    "         pow(math.sin(dLon / 2), 2) * \n",
    "         math.cos(lat1) * math.cos(lat2))\n",
    "# Earth radius in kilometers\n",
    "    rad = 6378\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return rad * c\n",
    "\n",
    "# Calculate the nearest warehouse and distance for each customer\n",
    "for index, row in dirty_data.iterrows():\n",
    "    # Get customer latitude\n",
    "    customer_lat = row['customer_lat']\n",
    "    # Get customer longitude\n",
    "    customer_long = row['customer_long']\n",
    "    # Initialize min distance to infinity\n",
    "    min_distance = float('inf')\n",
    "     # Initialize nearest warehouse name\n",
    "    nearest_warehouse_name = None  # 初始化最近仓库名称为空\n",
    "    \n",
    "    # Calculate the distance to each warehouse\n",
    "    for name in lat:\n",
    "        dist = round(haversine(customer_lat, customer_long, lat[name], lon[name]), 4)\n",
    "        # Update min distance and nearest warehouse if closer\n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            nearest_warehouse_name = name\n",
    "    \n",
    "    # Update dirty_data with the nearest warehouse and distance\n",
    "    dirty_data.at[index, 'nearest_warehouse'] = nearest_warehouse_name\n",
    "    dirty_data.at[index, 'distance_to_nearest_warehouse'] = min_distance\n",
    "\n",
    "# Count the occurrences of each warehouse in the 'nearest_warehouse' column\n",
    "warehouse_count = dirty_data['nearest_warehouse'].value_counts()\n",
    "# Print the result\n",
    "print(warehouse_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.3. Fix the shopping_cart, order_price, distance_to_nearest_warehouse and nearest_warehouse<a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shopping_cart & order_price\n",
    "for index, row in missing_data_Nickolson.iterrows():\n",
    "    \n",
    "    # 将字符串形式的 shopping_cart 列转换为 Python 列表\n",
    "    shopping_cart = eval(row['shopping_cart'])\n",
    "    \n",
    "    # 输出解析后的购物车内容、类型及其元素\n",
    "    print(\"购物车内容：\", shopping_cart)           # 输出整个购物车内容\n",
    "    print(\"购物车类型：\", type(shopping_cart))     # 输出购物车的类型，应该是列表\n",
    "    print(\"第一个商品：\", shopping_cart[0])        # 输出第一个商品的信息\n",
    "    print(\"第二个商品：\", shopping_cart[1])        # 输出第二个商品的信息\n",
    "    \n",
    "    # 进一步访问列表中每个商品的具体信息\n",
    "    print(\"第一个商品名称：\", shopping_cart[0][0])  # 输出第一个商品的名称\n",
    "    print(\"第二个商品数量：\", shopping_cart[1][1])  # 输出第二个商品的数量\n",
    "    \n",
    "    # 为了只运行一次循环，使用 break 停止遍历\n",
    "    break\n",
    "  \n",
    "item_types = set()  # 初始化一个集合来存储商品名称\n",
    "\n",
    "for index, row in missing_data_Nickolson.iterrows():\n",
    "    shopping_cart = ast.literal_eval(row['shopping_cart'])  # 解析购物车字符串\n",
    "    for item in shopping_cart:\n",
    "        item_types.add(item[0])  # 将商品名称添加到集合中\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义系数矩阵 a\n",
    "a = np.array([[1, 2], [3, 5]])\n",
    "\n",
    "# 定义结果向量 b\n",
    "b = np.array([1, 2])\n",
    "\n",
    "# 使用 numpy 的 linalg.solve() 解决线性方程组 a * x = b\n",
    "x = np.linalg.solve(a, b)\n",
    "\n",
    "# 输出解 x\n",
    "print(\"方程组的解：\", x)\n",
    "\n",
    "order total & order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "task1_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
