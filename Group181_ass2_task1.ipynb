{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCbAmQ47iqK4"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    \n",
    "# FIT5196 Task 1 in Assessment 2\n",
    "#### Student Name: Deshui Yu      Liangjing Yang\n",
    "#### Student ID: 34253599      34060871\n",
    "\n",
    "Date: 28/09/2024\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IjBFqYK4iqK5"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "    \n",
    "## Table of Contents\n",
    "\n",
    "</div>    \n",
    "\n",
    "[1. Introduction](#Intro) <br>\n",
    "[2. Importing Libraries](#libs) <br>\n",
    "[3. Examining Patent Files](#examine) <br>\n",
    "[4. Loading and Parsing Files](#load) <br>\n",
    "$\\;\\;\\;\\;$[4.1. Defining Regular Expressions](#Reg_Exp) <br>\n",
    "$\\;\\;\\;\\;$[4.2. Reading Files](#Read) <br>\n",
    "$\\;\\;\\;\\;$[4.3. Whatever else](#latin) <br>\n",
    "[5. Writing to CSV/JSON File](#write) <br>\n",
    "$\\;\\;\\;\\;$[5.1. Verification - using the sample files](#test_xml) <br>\n",
    "[6. Summary](#summary) <br>\n",
    "[7. References](#Ref) <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcbqK3KliqK6"
   },
   "source": [
    "-------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEFdSCIUiqK6"
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 1.  Introduction  <a class=\"anchor\" name=\"Intro\"></a>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project involves cleansing and analyzing a retail transactional dataset from DigiCO, an online electronics store in Melbourne. The task is to detect and fix errors, impute missing values, and remove outliers using exploratory data analysis (EDA). Cleaned data will be saved in the required output files, and the process will be documented in the final report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "## 2.  Importing Libraries  <a class=\"anchor\" name=\"libs\"></a>\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:\n",
    "\n",
    "* **re:** to define and use regular expressions\n",
    "* **pandas:** to manage and analyze data.\n",
    "* **datetime** to handle dates and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "\n",
    "## 3.  Examining Raw Data <a class=\"anchor\" name=\"examine\"></a>\n",
    "\n",
    " </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以看到这三个文件都包含以下数据列：order_id、customer_id、date、nearest_warehouse、shopping_cart、order_price、delivery_charges、customer_lat、customer_long、coupon_discount、order_total、season、is_expedited_delivery、distance_to_nearest_warehouse、latest_customer_review 和 is_happy_customer。其中，coupon_discount、delivery_charges、shopping_cart 中的商品数量、order_id、customer_id 和 latest_customer_review 是没有错误的数据。在 Group181_missing_data.csv 文件中，is_happy_customer 列的数据缺失，其数据类型为 float64；在 Group181_dirty_data.csv 中存在错误数据；而在 Group181_outlier_data.csv 文件中则有异常数据。\n",
    "通过数据逻辑我们可以发现，date和season数据是有关系的，customer_lat and customer_long 和 distance_to_nearest_warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirty_file_path = 'Group181_dirty_data.csv'\n",
    "dirty_data = pd.read_csv(dirty_file_path)\n",
    "print(dirty_data.info())\n",
    "print(dirty_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_file_path = 'Group181_missing_data.csv'\n",
    "missing_data = pd.read_csv(missing_file_path)\n",
    "print(missing_data.info())\n",
    "print(missing_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> \n",
    "\n",
    "## 4.  Detect and fix errors in dirty_data <a class=\"anchor\" name=\"load\"></a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.1. Fix the date and season <a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through examining the current data, the issue was identified as the date values, which were supposed to be in the YYYY-MM-DD format, mistakenly being formatted as YYYY-DD-MM and DD-MM-YYYY. Additionally, based on logical reasoning, the date and season are highly correlated, so after fixing the date data, the season data should also be corrected accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check and mark invalid dates\n",
    "# reference from chatGPT\n",
    "# Temporarily convert the 'date' column to datetime format, marking invalid dates as NaT (Not a Time)\n",
    "temp_dates = pd.to_datetime(dirty_data['date'], errors='coerce')\n",
    "# Find the invalid date\n",
    "invalid_dates_temp = dirty_data[temp_dates.isna()]\n",
    "print(invalid_dates_temp[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_dates = []\n",
    "# Loop through each date string in the 'date' column\n",
    "for date_str in dirty_data['date']:\n",
    "    parts = date_str.split('-')\n",
    "    # Check if format is DD-MM-YYYY and fix to YYYY-MM-DD\n",
    "    if len(parts) == 3 and int(parts[0]) <= 12 and int(parts[1]) <= 31:\n",
    "        fixed_dates.append(f'{parts[2]}-{parts[1]}-{parts[0]}')  # DD-MM-YYYY -> YYYY-MM-DD\n",
    "    # Check if format is YYYY-DD-MM and fix to YYYY-MM-DD\n",
    "    elif len(parts) == 3 and int(parts[1]) > 12 and int(parts[2]) <= 12:\n",
    "        fixed_dates.append(f'{parts[0]}-{parts[2]}-{parts[1]}')  # YYYY-DD-MM -> YYYY-MM-DD\n",
    "    # Keep original format for other cases\n",
    "    else:\n",
    "        fixed_dates.append(date_str)\n",
    "# Convert the fixed dates back to datetime format and replace the original 'date' column\n",
    "dirty_data['date'] = pd.to_datetime(fixed_dates, errors='coerce')\n",
    "print(dirty_data['date'].type())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we know that the date and season data are logically related, once the date data has been corrected, the season data should also be updated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_seasons = []\n",
    "# Modify the season data based on the fixed date\n",
    "for date in dirty_data['date']:\n",
    "    month = date.month # Extract the month\n",
    "    if month in [9, 10, 11]:\n",
    "        fixed_seasons.append('Spring')\n",
    "    elif month in [12, 1, 2]:\n",
    "        fixed_seasons.append('Summer')\n",
    "    elif month in [3, 4, 5]:\n",
    "        fixed_seasons.append('Autumn')\n",
    "    elif month in [6, 7, 8]:\n",
    "        fixed_seasons.append('Winter')\n",
    "# Replace the original season column with the generated fixed_seasons\n",
    "dirty_data['season'] = fixed_seasons\n",
    "\n",
    "print(dirty_data[['date', 'season']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "### 4.2. Fix the customer_lat, customer_long, distance_to_nearest_warehouse and nearest_warehouse<a class=\"anchor\" name=\"Reg_Exp\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this business is based in Melbourne, the correct values for latitude and longitude should be Latitude: -37.8136° and Longitude: 144.9631°. However, I have discovered some incorrect data where the latitude and longitude values were swapped. After correcting these errors, the distance_to_nearest_warehouse and nearest_warehouse fields, which are calculated based on the latitude and longitude, may also need to be fixed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for latitude greater than 0 (latitude in Australia should be less than 0)\n",
    "lat_issue = dirty_data.loc[dirty_data['customer_lat'] > 0]\n",
    "print(lat_issue[['customer_lat', 'customer_long']])\n",
    "\n",
    "# Check for longitude less than 0 (longitude in Australia should be greater than 0)\n",
    "long_issue = dirty_data.loc[dirty_data['customer_long'] < 0]\n",
    "print(long_issue[['customer_lat', 'customer_long']])\n",
    "\n",
    "# Condition to find rows where either longitude < 0 or latitude > 0\n",
    "condition = (dirty_data['customer_long'] < 0) | (dirty_data['customer_lat'] > 0)\n",
    "\n",
    "# Select the rows matching the condition and swap the latitude and longitude\n",
    "#reference from chatGPT\n",
    "dirty_data.loc[condition, ['customer_lat', 'customer_long']] = \\\n",
    "    dirty_data.loc[condition, ['customer_long', 'customer_lat']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data after fixing latitude and longitude:\n",
      "   customer_lat  customer_long\n",
      "0    -37.822570     144.952745\n",
      "1    -37.818625     144.985920\n",
      "2    -37.824845     144.957647\n",
      "3    -37.809950     144.950436\n",
      "4    -37.800566     144.952814\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    dLat = (lat2 - lat1) * math.pi / 180.0  # 将纬度差转换为弧度\n",
    "    dLon = (lon2 - lon1) * math.pi / 180.0  # 将经度差转换为弧度\n",
    "    lat1 = (lat1) * math.pi / 180.0         # 将起点纬度转换为弧度\n",
    "    lat2 = (lat2) * math.pi / 180.0         # 将终点纬度转换为弧度\n",
    "    \n",
    "    # Haversine公式中的a值，计算两个角度之间的弧长\n",
    "    a = (pow(math.sin(dLat / 2), 2) + \n",
    "         pow(math.sin(dLon / 2), 2) * \n",
    "         math.cos(lat1) * math.cos(lat2))\n",
    "    \n",
    "    rad = 6378  # 地球半径，单位为公里\n",
    "    c = 2 * math.asin(math.sqrt(a))  # 计算大圆距离\n",
    "    return rad * c  # 返回距离，单位为公里"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warehouse = pd.read_csv(\"warehouses.csv\")\n",
    "lat = dict(zip(warehouse.names, warehouse.lat))\n",
    "lon = dict(zip(warehouse.names, warehouse.lon))\n",
    "\n",
    "for index, row in missing_data.iterrows():\n",
    "    customer_lat = row['customer_lat']  # 获取客户的纬度\n",
    "    customer_long = row['customer_long']  # 获取客户的经度\n",
    "    \n",
    "    min_distance = float('inf')  # 初始化最小距离为无穷大\n",
    "    nearest_name = None  # 初始化最近仓库名称为空\n",
    "    \n",
    "    # 遍历所有仓库，计算客户与每个仓库的距离\n",
    "    for name in lat:\n",
    "        dist = round(haversine(customer_lat, customer_long, lat[name], lon[name]), 4)  # 计算客户到仓库的距离\n",
    "        if dist < min_distance:  # 如果距离更小，更新最小距离和最近仓库名称\n",
    "            min_distance = dist\n",
    "            nearest_name = name\n",
    "    \n",
    "    # 将计算得到的最近距离和仓库名称写回 missing_data 数据集中\n",
    "    missing_data.at[index, 'nearest_distance'] = min_distance\n",
    "    missing_data.at[index, 'nearest_name'] = nearest_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shopping_cart & order_price\n",
    "for index, row in missing_data_Nickolson.iterrows():\n",
    "    \n",
    "    # 将字符串形式的 shopping_cart 列转换为 Python 列表\n",
    "    shopping_cart = eval(row['shopping_cart'])\n",
    "    \n",
    "    # 输出解析后的购物车内容、类型及其元素\n",
    "    print(\"购物车内容：\", shopping_cart)           # 输出整个购物车内容\n",
    "    print(\"购物车类型：\", type(shopping_cart))     # 输出购物车的类型，应该是列表\n",
    "    print(\"第一个商品：\", shopping_cart[0])        # 输出第一个商品的信息\n",
    "    print(\"第二个商品：\", shopping_cart[1])        # 输出第二个商品的信息\n",
    "    \n",
    "    # 进一步访问列表中每个商品的具体信息\n",
    "    print(\"第一个商品名称：\", shopping_cart[0][0])  # 输出第一个商品的名称\n",
    "    print(\"第二个商品数量：\", shopping_cart[1][1])  # 输出第二个商品的数量\n",
    "    \n",
    "    # 为了只运行一次循环，使用 break 停止遍历\n",
    "    break\n",
    "  \n",
    "item_types = set()  # 初始化一个集合来存储商品名称\n",
    "\n",
    "for index, row in missing_data_Nickolson.iterrows():\n",
    "    shopping_cart = ast.literal_eval(row['shopping_cart'])  # 解析购物车字符串\n",
    "    for item in shopping_cart:\n",
    "        item_types.add(item[0])  # 将商品名称添加到集合中\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 定义系数矩阵 a\n",
    "a = np.array([[1, 2], [3, 5]])\n",
    "\n",
    "# 定义结果向量 b\n",
    "b = np.array([1, 2])\n",
    "\n",
    "# 使用 numpy 的 linalg.solve() 解决线性方程组 a * x = b\n",
    "x = np.linalg.solve(a, b)\n",
    "\n",
    "# 输出解 x\n",
    "print(\"方程组的解：\", x)\n",
    "\n",
    "order total & order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "task1_xxxxxxx.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
